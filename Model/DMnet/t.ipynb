{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_creation import *\n",
    "\n",
    "def bin_signal(x, bin_size=10):\n",
    "    # x shape: (7500,)\n",
    "    return x.reshape(-1, bin_size).sum(axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2648.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19860000 / 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved images to /scratch/Arya/Processed_MAYO/0_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/1_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/2_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/3_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/4_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/5_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/6_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/7_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/14_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/8_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/9_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/16_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/17_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/18_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/19_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/20_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/21_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/23_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/10_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/11_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/12_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/13_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/15_data_images.npy\n",
      "Processed and saved images to /scratch/Arya/Processed_MAYO/22_data_images.npy\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "path = \"/scratch/Arya/Processed_MAYO/\"\n",
    "\n",
    "# iterate over i_data.npy files and bin them and create images then save them in the same folder under then name i_data_images.npy\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    # the files should have format 3_data.npy or 4_data.npy \n",
    "    for file in files:\n",
    "        if file.endswith(\"_data.npy\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            data = np.load(file_path)  # shape (1, a fuck ton)\n",
    "\n",
    "            length = data.shape[-1] // 7500 * 7500\n",
    "\n",
    "            data = data.reshape(-1, 7500)\n",
    "\n",
    "            binned_data = np.array([bin_signal(x) for x in data])  # shape (N, 750)\n",
    "            \n",
    "            images = [recurrence_image(data) for data in binned_data]  # shape (N, 28, 28)\n",
    "            save_path = os.path.join(root, file.replace(\"_data.npy\", \"_data_images.npy\"))\n",
    "            np.save(save_path, images)\n",
    "            print(f\"Processed and saved images to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 128, 512])\n",
      "torch.Size([256, 16, 128, 32])\n",
      "torch.Size([256, 32, 128, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class HardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Hardware-friendly sigmoid approximation using ReLU + clip.\n",
    "    Forward: y = clip(alpha * x + 0.5, 0, 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(torch.relu(self.alpha * x + 0.5), 0.0, 1.0)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.sigmoid = HardSigmoid(alpha=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.sigmoid = HardSigmoid(alpha=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class cbam_block(nn.Module):\n",
    "\n",
    "    def __init__(self, channel, ratio=4, kernel_size=3):\n",
    "        super(cbam_block, self).__init__()\n",
    "        self.channelattention = ChannelAttention(channel, ratio=ratio)\n",
    "        self.spatialattention = SpatialAttention(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channelattention(x)\n",
    "        x = x * self.spatialattention(x)\n",
    "        return x\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "\n",
    "    # input shape: [batch_size, channels, H, W]\n",
    "    batch, channels, height, width = x.size()\n",
    "    channels_per_group = channels // groups\n",
    "    x = x.view(batch, groups, channels_per_group, height, width)\n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "    x = x.view(batch, channels, height, width)\n",
    "    return x\n",
    "\n",
    "class ChannelShuffle(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, groups):\n",
    "        super(ChannelShuffle, self).__init__()\n",
    "        if channels % groups != 0:\n",
    "            raise ValueError(\"The number of channels must be divisible by the number of groups.\")\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, x):\n",
    "        return channel_shuffle(x, self.groups)\n",
    "\n",
    "def Computing_mean(x, mask):\n",
    "\n",
    "    mask = torch.count_nonzero(mask, dim=2)\n",
    "    mask = torch.unsqueeze(mask, dim=2)\n",
    "    x = x.sum(dim=2, keepdim=True)\n",
    "    x = x / mask\n",
    "    return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, F1: int, classes_num: int, D: int = 2):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        self.drop_out = 0.25\n",
    "\n",
    "        self.att = cbam_block(D * F1)\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.ZeroPad2d((7, 7, 0, 0)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=F1,\n",
    "                kernel_size=(1, 16),\n",
    "                stride=(1, 2),\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((1, 8))\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.ZeroPad2d((7, 7, 0, 0)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=F1,\n",
    "                out_channels=F1,\n",
    "                kernel_size=(1, 16),\n",
    "                stride=(1, 2),\n",
    "                bias=False,\n",
    "                groups=F1\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=F1,\n",
    "                out_channels=D * F1,\n",
    "                kernel_size=(1, 1),\n",
    "                stride=(1, 1),\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(D * F1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=D * F1,\n",
    "                out_channels=D * F1,\n",
    "                kernel_size=(3, 1),\n",
    "                stride=(1, 1),\n",
    "                groups=D * F1,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=D * F1,\n",
    "                out_channels=D * D * F1,\n",
    "                kernel_size=(1, 1),\n",
    "                stride=(1, 1),\n",
    "                groups=4,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(D * D * F1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ChannelShuffle(D * D * F1, 4),\n",
    "        )\n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.ZeroPad2d((4, 3, 0, 0)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=D * D * F1,\n",
    "                out_channels=D * D * F1,\n",
    "                kernel_size=(1, 8),\n",
    "                stride=(1, 1),\n",
    "                groups=D * D * F1,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(D * D * F1),\n",
    "            nn.Conv2d(\n",
    "                in_channels=D * D * F1,\n",
    "                out_channels=D * D * D * F1,\n",
    "                kernel_size=(1, 1),\n",
    "                stride=(1, 1),\n",
    "                groups=4,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(D * D * D * F1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((1, 16))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.drop_out),\n",
    "            nn.LazyLinear(classes_num)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, ssp = None):\n",
    "\n",
    "        x = x.unsqueeze(1) # (Batch, 1, 128, 512)\n",
    "\n",
    "        mask = torch.abs(x).sum(dim=3, keepdim=True)\n",
    "        mask = (mask > 0).type(torch.float)\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        x = self.block_1(x)\n",
    "        print(x.shape)\n",
    "        x = self.block_2(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = x * mask\n",
    "        x1 = Computing_mean(x, mask)\n",
    "        x2 = torch.norm(x, p=2, dim=2, keepdim=True)\n",
    "        x3 = torch.norm(x, p=np.inf, dim=2, keepdim=True)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], 2)\n",
    "        x = self.att(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = CNN(F1=16, classes_num=2, D=2)\n",
    "A = torch.randn(256, 128, 512)\n",
    "model(A).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs= 256                  #sampling frequency\n",
    "channel= 128              #number of electrode\n",
    "num_input= 1             #number of channel picture (for EEG signal is always : 1)\n",
    "num_class= 2             #number of classes \n",
    "signal_length = 512      #number of sample in each tarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= 'cpu'\n",
    "kernel_size_1= (1,round(fs/2)) \n",
    "kernel_size_2= (channel, 1)\n",
    "kernel_size_3= (1, round(fs/8))\n",
    "kernel_size_4= (1, 1)\n",
    "\n",
    "kernel_avgpool_1= (1,4)\n",
    "kernel_avgpool_2= (1,8)\n",
    "dropout_rate= 0.2\n",
    "\n",
    "ks0= int(round((kernel_size_1[0]-1)/2))\n",
    "ks1= int(round((kernel_size_1[1]-1)/2))\n",
    "kernel_padding_1= (ks0, ks1-1)\n",
    "ks0= int(round((kernel_size_3[0]-1)/2))\n",
    "ks1= int(round((kernel_size_3[1]-1)/2))\n",
    "kernel_padding_3= (ks0, ks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module): \n",
    "    def __init__(self, F1, D):\n",
    "        super().__init__()\n",
    "        F2= F1*D\n",
    "        # layer 1\n",
    "        self.conv2d = nn.Conv2d(num_input, F1, kernel_size_1, padding=kernel_padding_1)\n",
    "        self.Batch_normalization_1 = nn.BatchNorm2d(F1)\n",
    "        # layer 2\n",
    "        self.Depthwise_conv2D = nn.Conv2d(F1, D*F1, kernel_size_2, groups= F1)\n",
    "        self.Batch_normalization_2 = nn.BatchNorm2d(D*F1)\n",
    "        self.Elu = nn.ELU()\n",
    "        self.Average_pooling2D_1 = nn.AvgPool2d(kernel_avgpool_1)\n",
    "        self.Dropout = nn.Dropout2d(dropout_rate)\n",
    "        # layer 3\n",
    "        self.Separable_conv2D_depth = nn.Conv2d( D*F1, D*F1, kernel_size_3,\n",
    "                                                padding=kernel_padding_3, groups= D*F1)\n",
    "        self.Separable_conv2D_point = nn.Conv2d(D*F1, F2, kernel_size_4)\n",
    "        self.Batch_normalization_3 = nn.BatchNorm2d(F2)\n",
    "        self.Average_pooling2D_2 = nn.AvgPool2d(kernel_avgpool_2)\n",
    "        # layer 4\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.Dense = nn.Linear(F2*round(signal_length/32), num_class)\n",
    "        self.Softmax = nn.Softmax(dim= 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        y = self.Batch_normalization_1(self.conv2d(x)) \n",
    "        print(y.shape)\n",
    "        y = self.Batch_normalization_2(self.Depthwise_conv2D(y))\n",
    "        y = self.Elu(y)\n",
    "        y = self.Dropout(self.Average_pooling2D_1(y))\n",
    "        y = self.Separable_conv2D_depth(y)\n",
    "        y = self.Batch_normalization_3(self.Separable_conv2D_point(y))\n",
    "        y = self.Elu(y)\n",
    "        y = self.Dropout(self.Average_pooling2D_2(y))\n",
    "        y = self.Flatten(y)\n",
    "        y = self.Dense(y)\n",
    "        y = self.Softmax(y)\n",
    "        \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 8, 128, 511])\n",
      "Number of parameters in the model: 6402\n"
     ]
    }
   ],
   "source": [
    "signal = torch.randn(256, 1, 128, 512)\n",
    "model = EEGNet(8, 3)\n",
    "\n",
    "model(signal).shape\n",
    "\n",
    "# count the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in the model: {count_parameters(model)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
